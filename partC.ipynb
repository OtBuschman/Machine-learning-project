{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfad026",
   "metadata": {},
   "source": [
    "# Part C - Tuning with GridSearch\n",
    "\n",
    "This notebook addresses questions 8-11 from the assignment:\n",
    "\n",
    "8. **GridSearch**: Use gridsearch to identify the best hyperparameters for the 4 models\n",
    "9. **Performance Estimation**: Fairly estimate the performance of the tuned models\n",
    "10. **Training Curve**: Include a training curve for SGD regressor convergence\n",
    "11. **Comparison**: Compare performance before and after hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df796b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Enable experimental features BEFORE importing IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9231f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create best pipeline from Part B\n",
    "print(\"Loading data and setting up best pipeline from Part B...\")\n",
    "\n",
    "# Load training data\n",
    "data = pd.read_csv('health_insurance_train.csv')\n",
    "X = data.drop('whrswk', axis=1)\n",
    "y = data['whrswk']\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Target variable (whrswk) - Mean: {y.mean():.2f}, Std: {y.std():.2f}\")\n",
    "\n",
    "# Define feature types\n",
    "numerical_feats = ['experience', 'kidslt6', 'kids618', 'husby']\n",
    "categorical_feats = ['hhi', 'whi', 'hhi2', 'education', 'race', 'hispanic', 'region']\n",
    "\n",
    "print(f\"\\nFeature types defined:\")\n",
    "print(f\"  Numerical: {len(numerical_feats)} features\")\n",
    "print(f\"  Categorical: {len(categorical_feats)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd365432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Part B results, we use the best pipeline\n",
    "# NOTE: Update this based on your Part B results!\n",
    "# For now, we'll implement both pipelines and use the best one\n",
    "\n",
    "def create_pipeline1():\n",
    "    \"\"\"Pipeline 1: Basic preprocessing\"\"\"\n",
    "    numerical_transf = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transf = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")), \n",
    "        ('encoder', OneHotEncoder())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transf, numerical_feats),\n",
    "            ('cat', categorical_transf, categorical_feats)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# Assuming Pipeline 1 was best from Part B (update based on your results)\n",
    "best_pipeline = create_pipeline1()\n",
    "X_for_training = X\n",
    "\n",
    "print(\"‚úÖ Best pipeline from Part B loaded\")\n",
    "print(\"   Using: Pipeline 1 (Basic preprocessing)\")\n",
    "print(\"\\nNote: If Pipeline 2 was better in your Part B, update this cell accordingly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline performance from Part B for comparison\n",
    "print(\"Calculating baseline performance (default hyperparameters)...\")\n",
    "\n",
    "# Define models with default hyperparameters\n",
    "models_default = {\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'SGD': SGDRegressor(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestRegressor(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Store baseline results\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in models_default.items():\n",
    "    # Create full pipeline\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', best_pipeline),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(full_pipeline, X_for_training, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    mae = -cv_scores.mean()\n",
    "    std = cv_scores.std()\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'std': std\n",
    "    }\n",
    "    \n",
    "    print(f\"  {name:<15} MAE: {mae:.4f} (+/- {std * 2:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Baseline performance calculated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbe05a",
   "metadata": {},
   "source": [
    "## Question 8: GridSearch for Hyperparameter Tuning\n",
    "\n",
    "**Approach:**\n",
    "- Use **systematic hyperparameter search** with GridSearchCV\n",
    "- Use **logarithmic scales** for parameters that can vary widely (as recommended in assignment hints)\n",
    "- Use **3-fold cross-validation** during grid search (balance between accuracy and computation time)\n",
    "- Use **reproducible settings** with fixed random_state\n",
    "\n",
    "**Hyperparameter Search Ranges:**\n",
    "\n",
    "### KNN Regressor:\n",
    "- `n_neighbors`: [3, 5, 7, 9, 11, 15, 20] - test different neighborhood sizes\n",
    "- `weights`: ['uniform', 'distance'] - uniform vs distance-weighted\n",
    "- `p`: [1, 2] - Manhattan (p=1) vs Euclidean (p=2) distance\n",
    "\n",
    "### SGD Regressor:\n",
    "- `alpha`: [0.0001, 0.001, 0.01, 0.1, 1.0] - regularization strength (logarithmic scale)\n",
    "- `learning_rate`: ['constant', 'optimal', 'invscaling', 'adaptive'] - learning rate schedules\n",
    "- `eta0`: [0.001, 0.01, 0.1, 1.0] - initial learning rate (logarithmic scale)\n",
    "- `max_iter`: [1000, 2000, 5000] - maximum iterations for convergence\n",
    "- `penalty`: ['l2', 'l1', 'elasticnet'] - regularization type\n",
    "\n",
    "### Random Forest:\n",
    "- `n_estimators`: [50, 100, 200, 300] - number of trees\n",
    "- `max_depth`: [None, 10, 20, 30] - maximum tree depth\n",
    "- `min_samples_split`: [2, 5, 10] - minimum samples to split\n",
    "- `min_samples_leaf`: [1, 2, 4] - minimum samples per leaf\n",
    "- `max_features`: ['sqrt', 'log2', None] - features to consider for splits\n",
    "\n",
    "### Decision Tree:\n",
    "- `max_depth`: [None, 5, 10, 15, 20, 25] - maximum tree depth\n",
    "- `min_samples_split`: [2, 5, 10, 20] - minimum samples to split\n",
    "- `min_samples_leaf`: [1, 2, 4, 8] - minimum samples per leaf\n",
    "- `criterion`: ['squared_error', 'friedman_mse', 'absolute_error'] - split quality measure\n",
    "- `splitter`: ['best', 'random'] - split strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8: Perform GridSearch for all models\n",
    "import time\n",
    "\n",
    "print(\"Question 8: Hyperparameter Tuning with GridSearchCV\")\n",
    "print(\"=\"*70)\n",
    "print(\"This will take several minutes...\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'KNN': {\n",
    "        'regressor__n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "        'regressor__weights': ['uniform', 'distance'],\n",
    "        'regressor__p': [1, 2]\n",
    "    },\n",
    "    'SGD': {\n",
    "        'regressor__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],  # Logarithmic scale\n",
    "        'regressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'regressor__eta0': [0.001, 0.01, 0.1, 1.0],  # Logarithmic scale\n",
    "        'regressor__max_iter': [1000, 2000, 5000],\n",
    "        'regressor__penalty': ['l2', 'l1', 'elasticnet']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'regressor__n_estimators': [50, 100, 200, 300],\n",
    "        'regressor__max_depth': [None, 10, 20, 30],\n",
    "        'regressor__min_samples_split': [2, 5, 10],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4],\n",
    "        'regressor__max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'regressor__max_depth': [None, 5, 10, 15, 20, 25],\n",
    "        'regressor__min_samples_split': [2, 5, 10, 20],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4, 8],\n",
    "        'regressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
    "        'regressor__splitter': ['best', 'random']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store tuned models and results\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "print(\"\\nPerforming GridSearch for each model...\")\n",
    "for name, param_grid in param_grids.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Tuning {name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create base model\n",
    "    if name == 'KNN':\n",
    "        base_model = KNeighborsRegressor()\n",
    "    elif name == 'SGD':\n",
    "        base_model = SGDRegressor(random_state=RANDOM_STATE)\n",
    "    elif name == 'Random Forest':\n",
    "        base_model = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "    elif name == 'Decision Tree':\n",
    "        base_model = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Create pipeline\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', best_pipeline),\n",
    "        ('regressor', base_model)\n",
    "    ])\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total_combinations = 1\n",
    "    for param_values in param_grid.values():\n",
    "        total_combinations *= len(param_values)\n",
    "    \n",
    "    print(f\"Testing {total_combinations} parameter combinations with 3-fold CV\")\n",
    "    print(f\"Total fits: {total_combinations * 3}\")\n",
    "    \n",
    "    # Grid search with 3-fold CV\n",
    "    grid_search = GridSearchCV(\n",
    "        model_pipeline,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_for_training, y)\n",
    "    \n",
    "    # Store results\n",
    "    tuned_models[name] = grid_search.best_estimator_\n",
    "    tuning_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': -grid_search.best_score_,  # Convert back to positive MAE\n",
    "        'tuning_time': time.time() - start_time,\n",
    "        'cv_results': grid_search.cv_results_\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {name} tuning completed in {tuning_results[name]['tuning_time']:.2f} seconds\")\n",
    "    print(f\"Best MAE (CV): {tuning_results[name]['best_score']:.4f}\")\n",
    "    print(f\"Best parameters:\")\n",
    "    for param, value in tuning_results[name]['best_params'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL HYPERPARAMETER TUNING COMPLETED\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221af1a3",
   "metadata": {},
   "source": [
    "## Question 9: Fair Performance Estimation of Tuned Models\n",
    "\n",
    "**Why this is a fair estimate:**\n",
    "1. **5-fold Cross-Validation** on the full dataset - provides robust performance estimate\n",
    "2. **Separate from tuning** - we use the best model from GridSearch and evaluate it independently\n",
    "3. **Multiple folds** - reduces variance in the estimate\n",
    "4. **Same evaluation method** as baseline - allows fair comparison\n",
    "\n",
    "**How we estimate performance:**\n",
    "- Use the best model from GridSearch\n",
    "- Apply 5-fold cross-validation on the full training dataset\n",
    "- Calculate mean and standard deviation of MAE across folds\n",
    "- This gives us an unbiased estimate of performance on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9: Evaluate tuned models with 5-fold CV\n",
    "print(\"Question 9: Fair Performance Estimation of Tuned Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tuned_performance = {}\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    # 5-fold cross-validation for fair estimate\n",
    "    cv_scores = cross_val_score(model, X_for_training, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    mae = -cv_scores.mean()\n",
    "    std = cv_scores.std()\n",
    "    \n",
    "    tuned_performance[name] = {\n",
    "        'MAE': mae,\n",
    "        'std': std,\n",
    "        'cv_scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae:.4f} (+/- {std * 2:.4f})\")\n",
    "    print(f\"  Individual CV scores: {[-score for score in cv_scores]}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tuned models evaluated with 5-fold CV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79dcb3",
   "metadata": {},
   "source": [
    "## Question 10: Training Curve for SGD Regressor\n",
    "\n",
    "**Purpose:** Demonstrate that the SGD regressor converges to a reasonable solution\n",
    "\n",
    "**What to show:**\n",
    "- Plot MAE vs epochs to show convergence\n",
    "- Verify that the loss decreases and stabilizes\n",
    "- Ensure enough iterations were used for convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 10: Create training curve for SGD Regressor\n",
    "print(\"Question 10: SGD Regressor Training Curve\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the best SGD model\n",
    "best_sgd = tuned_models['SGD']\n",
    "sgd_regressor = best_sgd.named_steps['regressor']\n",
    "\n",
    "print(f\"Best SGD parameters:\")\n",
    "for param, value in tuning_results['SGD']['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Train SGD with verbose output to track convergence\n",
    "print(\"\\nTraining SGD to track convergence...\")\n",
    "\n",
    "# Create a new SGD model with the best parameters and verbose output\n",
    "sgd_params = {k.replace('regressor__', ''): v for k, v in tuning_results['SGD']['best_params'].items()}\n",
    "sgd_for_curve = SGDRegressor(**sgd_params, random_state=RANDOM_STATE, verbose=0)\n",
    "\n",
    "# Manually track training progress\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocess the data first\n",
    "X_preprocessed = best_pipeline.fit_transform(X_for_training)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_preprocessed, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Train with partial_fit to track progress\n",
    "max_iter = sgd_params.get('max_iter', 1000)\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "epochs = []\n",
    "\n",
    "# Initialize the model\n",
    "sgd_for_curve.partial_fit(X_train[:100], y_train[:100])\n",
    "\n",
    "# Train epoch by epoch\n",
    "for epoch in range(1, max_iter + 1):\n",
    "    # Train on full training set\n",
    "    sgd_for_curve.partial_fit(X_train, y_train)\n",
    "    \n",
    "    # Track errors every 50 epochs\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        train_pred = sgd_for_curve.predict(X_train)\n",
    "        val_pred = sgd_for_curve.predict(X_val)\n",
    "        \n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        val_mae = mean_absolute_error(y_val, val_pred)\n",
    "        \n",
    "        train_errors.append(train_mae)\n",
    "        val_errors.append(val_mae)\n",
    "        epochs.append(epoch)\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"  Epoch {epoch}: Train MAE = {train_mae:.4f}, Val MAE = {val_mae:.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_errors, 'b-', label='Training MAE', linewidth=2)\n",
    "plt.plot(epochs, val_errors, 'r-', label='Validation MAE', linewidth=2)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('MAE', fontsize=12)\n",
    "plt.title('SGD Regressor: Training Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom in on later epochs to show convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(epochs) > 10:\n",
    "    start_idx = len(epochs) // 2\n",
    "    plt.plot(epochs[start_idx:], train_errors[start_idx:], 'b-', label='Training MAE', linewidth=2)\n",
    "    plt.plot(epochs[start_idx:], val_errors[start_idx:], 'r-', label='Validation MAE', linewidth=2)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('MAE', fontsize=12)\n",
    "    plt.title('SGD Regressor: Convergence (Second Half)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Training curve shows SGD converged\")\n",
    "print(f\"Final Training MAE: {train_errors[-1]:.4f}\")\n",
    "print(f\"Final Validation MAE: {val_errors[-1]:.4f}\")\n",
    "print(f\"Convergence achieved: {'Yes' if abs(val_errors[-1] - val_errors[-2]) < 0.01 else 'Check if more iterations needed'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948813ad",
   "metadata": {},
   "source": [
    "## Question 11: Compare Performance Before and After Hyperparameter Tuning\n",
    "\n",
    "**Comparison approach:**\n",
    "- Show side-by-side comparison of MAE before and after tuning\n",
    "- Calculate improvement percentage for each model\n",
    "- Identify which models benefited most from tuning\n",
    "- Determine the best overall model after tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 11: Compare performance before and after tuning\n",
    "print(\"Question 11: Performance Comparison - Before vs After Tuning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "print(f\"\\n{'Model':<15} | {'Before (MAE)':<15} | {'After (MAE)':<15} | {'Improvement':<15} | {'Status':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "improvements = {}\n",
    "\n",
    "for name in baseline_results.keys():\n",
    "    before_mae = baseline_results[name]['MAE']\n",
    "    after_mae = tuned_performance[name]['MAE']\n",
    "    \n",
    "    improvement = ((before_mae - after_mae) / before_mae * 100)\n",
    "    improvements[name] = improvement\n",
    "    \n",
    "    status = \"‚úÖ Better\" if improvement > 0 else \"‚ö†Ô∏è Worse\"\n",
    "    \n",
    "    print(f\"{name:<15} | {before_mae:<15.4f} | {after_mae:<15.4f} | {improvement:>+13.2f}% | {status:<10}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_improvement_model = max(improvements.keys(), key=lambda x: improvements[x])\n",
    "worst_improvement_model = min(improvements.keys(), key=lambda x: improvements[x])\n",
    "\n",
    "print(f\"\\nMost improved model: {best_improvement_model} ({improvements[best_improvement_model]:+.2f}%)\")\n",
    "print(f\"Least improved model: {worst_improvement_model} ({improvements[worst_improvement_model]:+.2f}%)\")\n",
    "\n",
    "avg_improvement = np.mean(list(improvements.values()))\n",
    "print(f\"Average improvement: {avg_improvement:+.2f}%\")\n",
    "\n",
    "# Find best overall model\n",
    "best_overall_model = min(tuned_performance.keys(), key=lambda x: tuned_performance[x]['MAE'])\n",
    "best_overall_mae = tuned_performance[best_overall_model]['MAE']\n",
    "\n",
    "print(f\"\\nüèÜ Best overall model after tuning: {best_overall_model}\")\n",
    "print(f\"   MAE: {best_overall_mae:.4f}\")\n",
    "print(f\"   Improvement over default: {improvements[best_overall_model]:+.2f}%\")\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart comparing before and after\n",
    "models = list(baseline_results.keys())\n",
    "before_maes = [baseline_results[m]['MAE'] for m in models]\n",
    "after_maes = [tuned_performance[m]['MAE'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, before_maes, width, label='Before Tuning', alpha=0.8, color='lightcoral')\n",
    "bars2 = ax1.bar(x + width/2, after_maes, width, label='After Tuning', alpha=0.8, color='lightgreen')\n",
    "\n",
    "ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Performance: Before vs After Hyperparameter Tuning', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Improvement percentage chart\n",
    "improvement_values = [improvements[m] for m in models]\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvement_values]\n",
    "\n",
    "bars3 = ax2.barh(models, improvement_values, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Percentage Improvement from Hyperparameter Tuning', fontsize=14, fontweight='bold')\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars3, improvement_values)):\n",
    "    ax2.text(val, i, f' {val:+.2f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection and autograder submission\n",
    "print(\"Final Model Selection for Autograder\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select best model\n",
    "final_model = tuned_models[best_overall_model]\n",
    "final_mae_estimate = tuned_performance[best_overall_model]['MAE']\n",
    "\n",
    "print(f\"Selected model: {best_overall_model}\")\n",
    "print(f\"Estimated MAE on unseen data: {final_mae_estimate:.4f}\")\n",
    "\n",
    "# Load autograder data\n",
    "print(\"\\nLoading autograder data...\")\n",
    "data_autograder = pd.read_csv('health_insurance_autograde.csv')\n",
    "print(f\"Autograder data shape: {data_autograder.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "predictions = final_model.predict(data_autograder)\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Min: {predictions.min():.4f}\")\n",
    "print(f\"  Max: {predictions.max():.4f}\")\n",
    "print(f\"  Mean: {predictions.mean():.4f}\")\n",
    "print(f\"  Std: {predictions.std():.4f}\")\n",
    "\n",
    "# Create submission file\n",
    "estimate_MAE_on_new_data = np.array([final_mae_estimate])\n",
    "predictions_autograder_data = predictions\n",
    "\n",
    "result = np.append(estimate_MAE_on_new_data, predictions_autograder_data)\n",
    "pd.DataFrame(result).to_csv(\"autograder_submission_partC.txt\", index=False, header=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission file created: 'autograder_submission_partC.txt'\")\n",
    "print(f\"File contains {len(result)} values (1 MAE estimate + {len(predictions)} predictions)\")\n",
    "\n",
    "# Verify submission\n",
    "submission_check = pd.read_csv(\"autograder_submission_partC.txt\", header=None)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Submission file shape: {submission_check.shape}\")\n",
    "print(f\"  First value (MAE estimate): {submission_check.iloc[0, 0]:.4f}\")\n",
    "print(f\"  Sample predictions: {submission_check.iloc[1:6, 0].values}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for autograder submission!\")\n",
    "print(f\"Best model: {best_overall_model}\")\n",
    "print(f\"Expected MAE: {final_mae_estimate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504baf4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
